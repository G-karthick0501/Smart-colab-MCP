{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMDohYONucbN",
        "outputId": "f8dc9950-47d9-4b5c-a2c6-8e978169d585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q flask pyngrok psutil GPUtil requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken **********************************************"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXKlYylGuvkK",
        "outputId": "f2c0f4bf-0b43-4a78-f1c8-b6354248c85e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify, send_file\n",
        "from pyngrok import ngrok\n",
        "import sys\n",
        "import os\n",
        "import psutil\n",
        "import traceback\n",
        "import time\n",
        "import threading\n",
        "import gc\n",
        "from io import StringIO\n",
        "from datetime import datetime\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Persistent namespace - variables survive between calls\n",
        "namespace = {\"__builtins__\": __builtins__}\n",
        "\n",
        "# Session tracking\n",
        "session_start = datetime.now()\n",
        "\n",
        "# =============================================================================\n",
        "# ENDPOINT: /probe - Understand Environment Capabilities\n",
        "# =============================================================================\n",
        "@app.route(\"/probe\", methods=[\"GET\"])\n",
        "def probe():\n",
        "    '''Return full environment capabilities and current state'''\n",
        "\n",
        "    # CPU & RAM\n",
        "    ram = psutil.virtual_memory()\n",
        "    cpu_percent = psutil.cpu_percent(interval=0.5)\n",
        "\n",
        "    # GPU (if available)\n",
        "    gpu_info = {\"available\": False}\n",
        "    try:\n",
        "        import GPUtil\n",
        "        gpus = GPUtil.getGPUs()\n",
        "        if gpus:\n",
        "            gpu = gpus[0]\n",
        "            gpu_info = {\n",
        "                \"available\": True,\n",
        "                \"name\": gpu.name,\n",
        "                \"memory_total_gb\": round(gpu.memoryTotal / 1024, 1),\n",
        "                \"memory_free_gb\": round(gpu.memoryFree / 1024, 1),\n",
        "                \"memory_used_gb\": round(gpu.memoryUsed / 1024, 1),\n",
        "                \"utilization_pct\": round(gpu.load * 100, 1)\n",
        "            }\n",
        "    except Exception as e:\n",
        "        gpu_info[\"error\"] = str(e)\n",
        "\n",
        "    # Disk\n",
        "    disk = psutil.disk_usage('/')\n",
        "\n",
        "    # Installed packages\n",
        "    packages = {}\n",
        "    pkg_list = [\n",
        "        'torch', 'tensorflow', 'sklearn', 'pandas', 'numpy',\n",
        "        'matplotlib', 'seaborn', 'scipy', 'PIL', 'cv2',\n",
        "        'transformers', 'datasets', 'joblib', 'pickle'\n",
        "    ]\n",
        "    for pkg in pkg_list:\n",
        "        try:\n",
        "            if pkg == 'PIL':\n",
        "                mod = __import__('PIL')\n",
        "            elif pkg == 'cv2':\n",
        "                mod = __import__('cv2')\n",
        "            elif pkg == 'sklearn':\n",
        "                mod = __import__('sklearn')\n",
        "            else:\n",
        "                mod = __import__(pkg)\n",
        "            packages[pkg] = getattr(mod, '__version__', 'installed')\n",
        "        except:\n",
        "            packages[pkg] = None\n",
        "\n",
        "    # Estimate safe limits\n",
        "    ram_gb = ram.available / (1024**3)\n",
        "    has_gpu = gpu_info.get(\"available\", False)\n",
        "\n",
        "    limits = {\n",
        "        \"max_dataset_gb\": round(ram_gb * 0.5, 1),\n",
        "        \"max_model_params_millions\": 500 if has_gpu else 10,\n",
        "        \"recommended_batch_size\": 128 if has_gpu else 32,\n",
        "        \"estimated_session_minutes_remaining\": max(0, 90 - (datetime.now() - session_start).seconds // 60),\n",
        "        \"checkpoint_interval_minutes\": 15,\n",
        "        \"safe_to_train_cnn\": has_gpu,\n",
        "        \"safe_to_load_large_dataset\": ram_gb > 4\n",
        "    }\n",
        "\n",
        "    # Platform detection\n",
        "    platform = \"unknown\"\n",
        "    if os.path.exists(\"/content\"):\n",
        "        platform = \"colab\"\n",
        "    elif os.environ.get(\"CODESPACES\"):\n",
        "        platform = \"codespaces\"\n",
        "    elif os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\"):\n",
        "        platform = \"kaggle\"\n",
        "\n",
        "    # What's currently loaded\n",
        "    user_vars = [k for k in namespace.keys() if not k.startswith('_')]\n",
        "\n",
        "    return jsonify({\n",
        "        \"compute\": {\n",
        "            \"cpu_cores\": psutil.cpu_count(),\n",
        "            \"cpu_usage_pct\": cpu_percent,\n",
        "            \"ram_total_gb\": round(ram.total / (1024**3), 1),\n",
        "            \"ram_available_gb\": round(ram.available / (1024**3), 1),\n",
        "            \"ram_used_pct\": ram.percent\n",
        "        },\n",
        "        \"gpu\": gpu_info,\n",
        "        \"storage\": {\n",
        "            \"disk_total_gb\": round(disk.total / (1024**3), 1),\n",
        "            \"disk_free_gb\": round(disk.free / (1024**3), 1)\n",
        "        },\n",
        "        \"packages\": packages,\n",
        "        \"limits\": limits,\n",
        "        \"platform\": platform,\n",
        "        \"session_uptime_minutes\": (datetime.now() - session_start).seconds // 60,\n",
        "        \"loaded_variables\": user_vars[:30],  # First 30\n",
        "        \"loaded_variables_count\": len(user_vars)\n",
        "    })\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ENDPOINT: /execute - Run Code with Resource Awareness\n",
        "# =============================================================================\n",
        "@app.route(\"/execute\", methods=[\"POST\"])\n",
        "def execute():\n",
        "    '''Execute Python code with memory monitoring and timeout awareness'''\n",
        "\n",
        "    data = request.json\n",
        "    code = data.get(\"code\", \"\")\n",
        "    timeout = data.get(\"timeout\", 300)\n",
        "\n",
        "    # Pre-execution checks\n",
        "    ram_before = psutil.virtual_memory()\n",
        "    ram_available_gb = ram_before.available / (1024**3)\n",
        "\n",
        "    # Warn if low memory\n",
        "    if ram_available_gb < 0.5:\n",
        "        return jsonify({\n",
        "            \"success\": False,\n",
        "            \"error\": f\"LOW MEMORY: Only {ram_available_gb:.1f}GB available. Call cleanup() first.\",\n",
        "            \"stdout\": \"\",\n",
        "            \"stderr\": \"\",\n",
        "            \"suggestion\": \"Run cleanup() or restart runtime to free memory\"\n",
        "        })\n",
        "\n",
        "    # Capture stdout/stderr\n",
        "    old_stdout, old_stderr = sys.stdout, sys.stderr\n",
        "    sys.stdout = stdout_capture = StringIO()\n",
        "    sys.stderr = stderr_capture = StringIO()\n",
        "\n",
        "    result = {\n",
        "        \"success\": True,\n",
        "        \"stdout\": \"\",\n",
        "        \"stderr\": \"\",\n",
        "        \"error\": None,\n",
        "        \"execution_time_sec\": 0,\n",
        "        \"memory_before_gb\": round(ram_available_gb, 2),\n",
        "        \"memory_after_gb\": 0,\n",
        "        \"memory_delta_mb\": 0,\n",
        "        \"warning\": None\n",
        "    }\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        # Execute in persistent namespace\n",
        "        exec(code, namespace)\n",
        "        result[\"stdout\"] = stdout_capture.getvalue()\n",
        "        result[\"stderr\"] = stderr_capture.getvalue()\n",
        "\n",
        "    except Exception as e:\n",
        "        result[\"success\"] = False\n",
        "        result[\"error\"] = traceback.format_exc()\n",
        "        result[\"stderr\"] = stderr_capture.getvalue()\n",
        "\n",
        "    finally:\n",
        "        sys.stdout, sys.stderr = old_stdout, old_stderr\n",
        "        result[\"execution_time_sec\"] = round(time.time() - start_time, 2)\n",
        "\n",
        "        # Post-execution memory check\n",
        "        ram_after = psutil.virtual_memory()\n",
        "        ram_after_gb = ram_after.available / (1024**3)\n",
        "        result[\"memory_after_gb\"] = round(ram_after_gb, 2)\n",
        "        result[\"memory_delta_mb\"] = round((ram_available_gb - ram_after_gb) * 1024, 1)\n",
        "\n",
        "        # Generate warnings\n",
        "        warnings = []\n",
        "        if ram_after.percent > 85:\n",
        "            warnings.append(f\"HIGH MEMORY ({ram_after.percent:.0f}%). Save results and consider cleanup().\")\n",
        "        if result[\"execution_time_sec\"] > 60:\n",
        "            warnings.append(f\"Long execution ({result['execution_time_sec']:.0f}s). Consider checkpointing.\")\n",
        "\n",
        "        if warnings:\n",
        "            result[\"warning\"] = \" | \".join(warnings)\n",
        "\n",
        "        # Truncate very long output\n",
        "        max_output = 50000\n",
        "        if len(result[\"stdout\"]) > max_output:\n",
        "            half = max_output // 2\n",
        "            result[\"stdout\"] = (\n",
        "                result[\"stdout\"][:half] +\n",
        "                f\"\\n\\n... OUTPUT TRUNCATED ({len(result['stdout'])} chars total) ...\\n\\n\" +\n",
        "                result[\"stdout\"][-half:]\n",
        "            )\n",
        "\n",
        "        if len(result.get(\"error\", \"\") or \"\") > 10000:\n",
        "            result[\"error\"] = result[\"error\"][:10000] + \"\\n... ERROR TRUNCATED ...\"\n",
        "\n",
        "    return jsonify(result)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ENDPOINT: /files/list - List Files in Directory\n",
        "# =============================================================================\n",
        "@app.route(\"/files/list\", methods=[\"GET\"])\n",
        "def list_files():\n",
        "    '''List files in a directory with sizes'''\n",
        "    path = request.args.get(\"path\", \"/content\")\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(path):\n",
        "            return jsonify({\"error\": f\"Path does not exist: {path}\"})\n",
        "\n",
        "        files = []\n",
        "        for f in os.listdir(path):\n",
        "            full_path = os.path.join(path, f)\n",
        "            try:\n",
        "                stat = os.stat(full_path)\n",
        "                is_dir = os.path.isdir(full_path)\n",
        "                files.append({\n",
        "                    \"name\": f,\n",
        "                    \"path\": full_path,\n",
        "                    \"is_dir\": is_dir,\n",
        "                    \"size_mb\": round(stat.st_size / (1024**2), 2) if not is_dir else None,\n",
        "                    \"modified\": datetime.fromtimestamp(stat.st_mtime).isoformat()\n",
        "                })\n",
        "            except:\n",
        "                files.append({\"name\": f, \"path\": full_path, \"error\": \"Could not stat\"})\n",
        "\n",
        "        # Sort: directories first, then by name\n",
        "        files.sort(key=lambda x: (not x.get(\"is_dir\", False), x[\"name\"].lower()))\n",
        "\n",
        "        return jsonify({\n",
        "            \"path\": path,\n",
        "            \"file_count\": len([f for f in files if not f.get(\"is_dir\")]),\n",
        "            \"dir_count\": len([f for f in files if f.get(\"is_dir\")]),\n",
        "            \"files\": files\n",
        "        })\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)})\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ENDPOINT: /files/download - Download File from Colab\n",
        "# =============================================================================\n",
        "@app.route(\"/files/download\", methods=[\"GET\"])\n",
        "def download_file():\n",
        "    '''Download a file from the remote environment'''\n",
        "    path = request.args.get(\"path\")\n",
        "\n",
        "    if not path:\n",
        "        return jsonify({\"error\": \"No path provided\"}), 400\n",
        "\n",
        "    if not os.path.exists(path):\n",
        "        return jsonify({\"error\": f\"File not found: {path}\"}), 404\n",
        "\n",
        "    if os.path.isdir(path):\n",
        "        return jsonify({\"error\": f\"Path is a directory, not a file: {path}\"}), 400\n",
        "\n",
        "    try:\n",
        "        return send_file(path, as_attachment=True)\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ENDPOINT: /files/read - Read text file content\n",
        "# =============================================================================\n",
        "@app.route(\"/files/read\", methods=[\"GET\"])\n",
        "def read_file():\n",
        "    '''Read content of a text file'''\n",
        "    path = request.args.get(\"path\")\n",
        "    max_size = int(request.args.get(\"max_size\", 100000))  # 100KB default\n",
        "\n",
        "    if not path or not os.path.exists(path):\n",
        "        return jsonify({\"error\": f\"File not found: {path}\"}), 404\n",
        "\n",
        "    try:\n",
        "        size = os.path.getsize(path)\n",
        "        if size > max_size:\n",
        "            return jsonify({\n",
        "                \"error\": f\"File too large ({size} bytes). Max: {max_size}\",\n",
        "                \"suggestion\": \"Use download endpoint instead\"\n",
        "            }), 400\n",
        "\n",
        "        with open(path, 'r') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        return jsonify({\"path\": path, \"size\": size, \"content\": content})\n",
        "    except UnicodeDecodeError:\n",
        "        return jsonify({\"error\": \"Binary file - use download endpoint\"}), 400\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ENDPOINT: /cleanup - Free Memory and Resources\n",
        "# =============================================================================\n",
        "@app.route(\"/cleanup\", methods=[\"POST\"])\n",
        "def cleanup():\n",
        "    '''Free memory by clearing namespace and running garbage collection'''\n",
        "\n",
        "    ram_before = psutil.virtual_memory()\n",
        "\n",
        "    # Get list of user variables to clear\n",
        "    keys_to_remove = [k for k in namespace.keys() if not k.startswith('_')]\n",
        "    removed_vars = keys_to_remove.copy()\n",
        "\n",
        "    # Clear namespace\n",
        "    for k in keys_to_remove:\n",
        "        try:\n",
        "            del namespace[k]\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Force garbage collection\n",
        "    gc.collect()\n",
        "\n",
        "    # Clear GPU memory if available\n",
        "    gpu_cleared = False\n",
        "    try:\n",
        "        import torch\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            gpu_cleared = True\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Clear any matplotlib figures\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "        plt.close('all')\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Remove temp files\n",
        "    temp_removed = 0\n",
        "    temp_extensions = ('.tmp', '.temp', '.log', '.pyc')\n",
        "    for f in os.listdir(\"/content\"):\n",
        "        if f.endswith(temp_extensions):\n",
        "            try:\n",
        "                os.remove(f\"/content/{f}\")\n",
        "                temp_removed += 1\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    # Force another gc\n",
        "    gc.collect()\n",
        "\n",
        "    ram_after = psutil.virtual_memory()\n",
        "    freed_mb = (ram_after.available - ram_before.available) / (1024**2)\n",
        "\n",
        "    return jsonify({\n",
        "        \"success\": True,\n",
        "        \"memory_freed_mb\": round(freed_mb, 1),\n",
        "        \"memory_available_now_gb\": round(ram_after.available / (1024**3), 2),\n",
        "        \"memory_used_pct\": ram_after.percent,\n",
        "        \"variables_cleared\": len(removed_vars),\n",
        "        \"cleared_variables\": removed_vars[:20],  # First 20\n",
        "        \"temp_files_removed\": temp_removed,\n",
        "        \"gpu_cache_cleared\": gpu_cleared\n",
        "    })\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ENDPOINT: /health - Quick Health Check\n",
        "# =============================================================================\n",
        "@app.route(\"/health\", methods=[\"GET\"])\n",
        "def health():\n",
        "    '''Quick health check'''\n",
        "    ram = psutil.virtual_memory()\n",
        "    return jsonify({\n",
        "        \"status\": \"ok\",\n",
        "        \"uptime_minutes\": (datetime.now() - session_start).seconds // 60,\n",
        "        \"memory_available_gb\": round(ram.available / (1024**3), 1),\n",
        "        \"memory_used_pct\": ram.percent\n",
        "    })\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ENDPOINT: /variables - List Variables in Namespace\n",
        "# =============================================================================\n",
        "@app.route(\"/variables\", methods=[\"GET\"])\n",
        "def list_variables():\n",
        "    '''List all variables currently in memory'''\n",
        "\n",
        "    vars_info = []\n",
        "    for name, value in namespace.items():\n",
        "        if name.startswith('_'):\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            var_type = type(value).__name__\n",
        "\n",
        "            # Get size/shape info\n",
        "            size_info = None\n",
        "            if hasattr(value, 'shape'):\n",
        "                size_info = str(value.shape)\n",
        "            elif hasattr(value, '__len__'):\n",
        "                size_info = f\"len={len(value)}\"\n",
        "\n",
        "            vars_info.append({\n",
        "                \"name\": name,\n",
        "                \"type\": var_type,\n",
        "                \"size\": size_info\n",
        "            })\n",
        "        except:\n",
        "            vars_info.append({\"name\": name, \"type\": \"unknown\", \"size\": None})\n",
        "\n",
        "    return jsonify({\n",
        "        \"count\": len(vars_info),\n",
        "        \"variables\": vars_info\n",
        "    })"
      ],
      "metadata": {
        "id": "8jRiTVB8u-dL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "\n",
        "def run_server():\n",
        "    app.run(port=5000, threaded=True)\n",
        "\n",
        "# Start Flask in background thread\n",
        "server_thread = threading.Thread(target=run_server)\n",
        "server_thread.daemon = True\n",
        "server_thread.start()\n",
        "\n",
        "# Wait for server to start\n",
        "import time\n",
        "time.sleep(2)\n",
        "\n",
        "# Create ngrok tunnel\n",
        "public_url = ngrok.connect(5000)\n",
        "\n",
        "# Print connection info\n",
        "print(\"=\" * 70)\n",
        "print(\"üöÄ SMART COLAB EXECUTOR IS READY!\")\n",
        "print(\"=\" * 70)\n",
        "print()\n",
        "print(f\"üì° PUBLIC URL: {public_url}\")\n",
        "print()\n",
        "print(\"=\" * 70)\n",
        "print(\"ENDPOINTS:\")\n",
        "print(\"  GET  /probe      - Check environment (RAM, GPU, packages)\")\n",
        "print(\"  POST /execute    - Run Python code\")\n",
        "print(\"  GET  /files/list - List files in directory\")\n",
        "print(\"  GET  /files/download?path=X - Download file\")\n",
        "print(\"  GET  /files/read?path=X - Read text file\")\n",
        "print(\"  POST /cleanup    - Free memory\")\n",
        "print(\"  GET  /health     - Quick status check\")\n",
        "print(\"  GET  /variables  - List loaded variables\")\n",
        "print(\"=\" * 70)\n",
        "print()\n",
        "print(\"üìã NEXT STEPS:\")\n",
        "print(\"  1. Copy the PUBLIC URL above\")\n",
        "print(\"  2. Update your local MCP server config\")\n",
        "print(\"  3. Restart Claude Desktop\")\n",
        "print(\"  4. Start chatting!\")\n",
        "print()\n",
        "print(\"‚ö†Ô∏è  Keep this notebook running! Closing it kills the server.\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNOq3-2MvDlp",
        "outputId": "e6859476-0513-431b-c068-793794096927"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Address already in use\n",
            "Port 5000 is in use by another program. Either identify and stop that program, or start the server with a different port.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "üöÄ SMART COLAB EXECUTOR IS READY!\n",
            "======================================================================\n",
            "\n",
            "üì° PUBLIC URL: NgrokTunnel: \"https://dustin-bimolecular-abbey.ngrok-free.dev\" -> \"http://localhost:5000\"\n",
            "\n",
            "======================================================================\n",
            "ENDPOINTS:\n",
            "  GET  /probe      - Check environment (RAM, GPU, packages)\n",
            "  POST /execute    - Run Python code\n",
            "  GET  /files/list - List files in directory\n",
            "  GET  /files/download?path=X - Download file\n",
            "  GET  /files/read?path=X - Read text file\n",
            "  POST /cleanup    - Free memory\n",
            "  GET  /health     - Quick status check\n",
            "  GET  /variables  - List loaded variables\n",
            "======================================================================\n",
            "\n",
            "üìã NEXT STEPS:\n",
            "  1. Copy the PUBLIC URL above\n",
            "  2. Update your local MCP server config\n",
            "  3. Restart Claude Desktop\n",
            "  4. Start chatting!\n",
            "\n",
            "‚ö†Ô∏è  Keep this notebook running! Closing it kills the server.\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}